{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_from_whisper, WillettDataset, mse_adjusted_temporal_gaussian_infonce_loss\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mK:\\ke\\dev\\WhisperBrain\\.venv\\src\\openai-whisper\\whisper\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_audio, log_mel_spectrogram, pad_or_trim\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\__init__.py:125\u001b[0m\n\u001b[0;32m    123\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 125\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from whisper.brain import initialize_from_whisper, WillettDataset, mse_adjusted_temporal_gaussian_infonce_loss\n",
    "import whisper\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Outline: Training WhisperBrain\n",
    "- Load data\n",
    "- Preprocess data\n",
    "- Train Contrastive Alignment of speech embeddings and brain embeddings\n",
    "- Train brain-to-text encoder-decoder.\n",
    "- Evaluate on test set\n",
    "- Save best checkpoint in a separate directory.\n",
    "- Transcribe holdout set and save to text file for submission (follow guidelines from competition)? Or do in other notebook?\n",
    "\n",
    "Reference for some finetuning code: https://colab.research.google.com/drive/1P4ClLkPmfsaKn2tBbRp0nVjGMRKR-EWz?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#model = initialize_from_whisper(name='tiny.en')\u001b[39;00m\n\u001b[0;32m      5\u001b[0m base_checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mke\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msta\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWillett&EtAl2023\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m(base_checkpoint_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoint_3459.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtiny.en\u001b[39m\u001b[38;5;124m'\u001b[39m, DEVICE)\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultilingual\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mis_multilingual\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish-only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mprod(p\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {'CUDA' if DEVICE.type == 'cuda' else 'CPU'}.\")\n",
    "#model = initialize_from_whisper(name='tiny.en')\n",
    "base_checkpoint_dir = \"K:\\ke\\sta\\data\\Willett&EtAl2023\\checkpoints\"\n",
    "model = load_checkpoint(base_checkpoint_dir + \"\\checkpoint_3459.pt\", 'tiny.en', DEVICE)\n",
    "model.to(DEVICE)\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "dataset = WillettDataset(\"K:\\ke\\sta\\data\\Willett&EtAl2023\\data\\Willett&EtAl2023.h5\", \"train\", device=DEVICE)\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "model.toggle_freeze('audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning Speech and Brain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop\n",
    "def train(model, data_loader, optimizer, alpha=0.5, iqr=20, temperature=0.07, neg_sample_prop=None):\n",
    "    model.train()\n",
    "    batches = len(data_loader)\n",
    "    i = 0\n",
    "    for brain_data, mels, texts, signal_lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass for brain and audio\n",
    "        brain_embeddings = model.embed_brain(brain_data)\n",
    "        audio_embeddings = model.embed_audio(mels)\n",
    "        # Compute losses\n",
    "        loss = mse_adjusted_temporal_gaussian_infonce_loss(brain_embeddings, audio_embeddings, signal_lengths, alpha=alpha, iqr=iqr, temperature=temperature, neg_sample_prop=neg_sample_prop)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        print(f\"Loss: {loss.item()} ({i}/{batches})         \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.32578706741333 (14/3460)          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miqr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sample_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, optimizer, alpha, iqr, temperature, neg_sample_prop)\u001b[0m\n\u001b[0;32m      4\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n\u001b[0;32m      5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass for brain and audio\u001b[39;49;00m\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mK:\\ke\\dev\\WhisperBrain\\whisper\\brain\\dataset.py:75\u001b[0m, in \u001b[0;36mWillettDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     73\u001b[0m raw_neural_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneural_data\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     74\u001b[0m signal_length \u001b[38;5;241m=\u001b[39m raw_neural_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 75\u001b[0m neural_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_neural_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_neural_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mholdout\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (neural_data, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, signal_length)\n",
      "File \u001b[1;32mK:\\ke\\dev\\WhisperBrain\\whisper\\brain\\dataset.py:93\u001b[0m, in \u001b[0;36mWillettDataset.preprocess_neural_data\u001b[1;34m(self, neural_data)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_neural_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, neural_data):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# neural data is of shape (T, 2, 2, 5, 8, 8)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     neural_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(neural_data)\n\u001b[1;32m---> 93\u001b[0m     neural_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smooth_threshold_crossings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneural_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     neural_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_noise_to_threshold_crossings(neural_data)\n\u001b[0;32m     95\u001b[0m     neural_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_neural_data(neural_data)\n",
      "File \u001b[1;32mK:\\ke\\dev\\WhisperBrain\\whisper\\brain\\dataset.py:115\u001b[0m, in \u001b[0;36mWillettDataset._smooth_threshold_crossings\u001b[1;34m(self, neural_data)\u001b[0m\n\u001b[0;32m    113\u001b[0m grid \u001b[38;5;241m=\u001b[39m [(a, sa, c, y, x) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m sa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m)]\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (a, sa, c, y, x) \u001b[38;5;129;01min\u001b[39;00m grid:\n\u001b[1;32m--> 115\u001b[0m     neural_data[:,a,sa,c,y,x]\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smooth_channel(neural_data[:,a,sa,c,y,x])\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m neural_data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, dataloader, optimizer, alpha=1, iqr=5, temperature=0.2, neg_sample_prop=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "checkpoint_dir = \"K:\\ke\\sta\\data\\Willett&EtAl2023\\checkpoints\"\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "torch.save(model.state_dict(), checkpoint_dir + f\"\\{timestamp}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Brain Encoder with Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50258, 50259, 50359, 50363, 22493,   749,   382,  2368,   382,   264,\n",
      "          3295, 50257]], device='cuda:0')\n",
      "('Boys as hot as the sun',)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(\"en\")\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "brain_data, mels, texts, signal_lengths = dataloader.__iter__().__next__()\n",
    "def clean_texts(texts):\n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        text = text.strip()\n",
    "        # remove all puncutation except apostrophes for truncated words\n",
    "        text = text.translate(str.maketrans(\"\", \"\", \".,!?()[]{};:<>\"))\n",
    "        # remove double spaces\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        new_texts.append(text.strip())\n",
    "    return tuple(new_texts)\n",
    "def encode_texts(texts):\n",
    "    # remove white space from start and end\n",
    "    sots = list(tokenizer.sot_sequence_including_notimestamps)\n",
    "    eot = [tokenizer.eot]\n",
    "    token_map = map(tokenizer.encode, texts)\n",
    "    token_array = []\n",
    "    for tokens in token_map:\n",
    "        token_array.append(sots + tokens + eot)\n",
    "    token_array = torch.tensor(token_array, dtype=torch.long, device=DEVICE)\n",
    "    return token_array\n",
    "cleaned_texts = clean_texts(texts)\n",
    "tokens = encode_texts(cleaned_texts)\n",
    "print(tokens)\n",
    "print(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "checkpoint_dir = \"K:\\ke\\sta\\data\\Willett&EtAl2023\\checkpoints\"\n",
    "seq2seq_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def wasserstein_loss(brain_logits, audio_logits):\n",
    "    brain_probs = F.softmax(brain_logits, dim=-1)\n",
    "    audio_probs = F.softmax(audio_logits, dim=-1)\n",
    "\n",
    "    # Cumulative distribution function (CDF) for each probability distribution\n",
    "    brain_cdf = torch.cumsum(brain_probs, dim=-1)\n",
    "    audio_cdf = torch.cumsum(audio_probs, dim=-1)\n",
    "\n",
    "    # Wasserstein distance is the L1 distance between the CDFs\n",
    "    return torch.mean(torch.abs(brain_cdf - audio_cdf))\n",
    "\n",
    "def logit_kld_loss(brain_logits, audio_logits):\n",
    "    brain_probs = F.log_softmax(brain_logits, dim=-1)\n",
    "    audio_probs = F.softmax(audio_logits, dim=-1)\n",
    "    return F.kl_div(brain_probs, audio_probs, reduction='batchmean')\n",
    "\n",
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "def alignment_loss(brain_embeddings, audio_embeddings):\n",
    "    # mean square error loss\n",
    "    mse_loss = F.mse_loss(brain_embeddings, audio_embeddings)\n",
    "    # cosine similarity along the time axis\n",
    "    brain_norm = F.normalize(brain_embeddings, dim=1)\n",
    "    audio_norm = F.normalize(audio_embeddings, dim=1)\n",
    "    cosine_sim = torch.einsum('nct,ncp->ntp', brain_norm, audio_norm)\n",
    "    # mean of the diagonal\n",
    "    time_alignment = cosine_sim.diagonal(dim1=1, dim2=2).mean()\n",
    "    # cosine similarity along the feature axis\n",
    "    brain_norm = F.normalize(brain_embeddings, dim=2)\n",
    "    audio_norm = F.normalize(audio_embeddings, dim=2)\n",
    "    cosine_sim = torch.einsum('nct,npt->nctp', brain_norm, audio_norm)\n",
    "    # mean of the diagonal\n",
    "    feature_alignment = cosine_sim.diagonal(dim1=2, dim2=3).mean()\n",
    "    return mse_loss, time_alignment, feature_alignment\n",
    "\n",
    "def train_step_with_scheduled_sampling(brain_data, mels, texts, optimizer, model, beta, sampling_prob=0.1):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass for brain and audio\n",
    "    brain_embeddings = model.embed_brain(brain_data)\n",
    "    audio_embeddings = model.embed_audio(mels)\n",
    "    # Compute losses\n",
    "    align_mse, align_cos_time, align_cos_feat = alignment_loss(brain_embeddings, audio_embeddings)\n",
    "    texts = clean_texts(texts)\n",
    "    tokens = encode_texts(texts)\n",
    "\n",
    "    # Start and end tokens\n",
    "    start_token = tokens[0, 0].item()\n",
    "    end_token = tokens[0, -1].item()\n",
    "\n",
    "    brain_seq2seq_loss = 0\n",
    "    audio_seq2seq_loss = 0\n",
    "\n",
    "    brain_generated_tokens = [start_token]\n",
    "    audio_generated_tokens = [start_token]\n",
    "\n",
    "    logit_loss = 0\n",
    "    logit_mse = 0\n",
    "    logit_kld = 0\n",
    "\n",
    "    for t in range(1, tokens.size(1)):\n",
    "        if random.random() < sampling_prob and t > 1:\n",
    "            brain_input = torch.tensor([brain_generated_tokens], device=brain_embeddings.device)\n",
    "            audio_input = torch.tensor([audio_generated_tokens], device=audio_embeddings.device)\n",
    "        else:\n",
    "            brain_input = tokens[:, :t]\n",
    "            audio_input = tokens[:, :t]\n",
    "\n",
    "        brain_logits = model.logits(brain_input, brain_embeddings)\n",
    "        audio_logits = model.logits(audio_input, audio_embeddings)\n",
    "\n",
    "        logit_loss += wasserstein_loss(brain_logits[:, -1, :], audio_logits[:, -1, :])\n",
    "        logit_mse += mse(brain_logits[:, -1, :], audio_logits[:, -1, :])\n",
    "        logit_kld += logit_kld_loss(brain_logits[:, -1, :], audio_logits[:, -1, :])\n",
    "\n",
    "        brain_next_token = brain_logits[:, -1].argmax(dim=-1).item()\n",
    "        audio_next_token = audio_logits[:, -1].argmax(dim=-1).item()\n",
    "\n",
    "        brain_generated_tokens.append(brain_next_token)\n",
    "        audio_generated_tokens.append(audio_next_token)\n",
    "\n",
    "        brain_seq2seq_loss += seq2seq_loss(brain_logits[:, -1, :], tokens[:, t])\n",
    "        audio_seq2seq_loss += seq2seq_loss(audio_logits[:, -1, :], tokens[:, t])\n",
    "\n",
    "    brain_seq2seq_loss /= tokens.size(1)\n",
    "    audio_seq2seq_loss /= tokens.size(1)\n",
    "    logit_loss /= tokens.size(1)\n",
    "    logit_mse /= tokens.size(1)\n",
    "    logit_kld /= tokens.size(1)\n",
    "\n",
    "    # get token agreement between brain and audio (allows for different length sequences)\n",
    "    end_a = min(len(brain_generated_tokens), len(audio_generated_tokens))\n",
    "    size_a = max(len(brain_generated_tokens), len(audio_generated_tokens))\n",
    "    token_disagreement = (sum([1 for b, a in zip(brain_generated_tokens[:end_a], audio_generated_tokens[:end_a]) if b != a]) + size_a - end_a) / size_a\n",
    "\n",
    "    end_t = min(len(brain_generated_tokens), tokens.size(1))\n",
    "    size_t = max(len(brain_generated_tokens), tokens.size(1))\n",
    "    token_error = (sum([1 for b, t in zip(brain_generated_tokens[:end_t], tokens[0, :end_t].tolist()) if b != t]) + size_t - end_t) / size_t\n",
    "\n",
    "    end_ta = min(len(audio_generated_tokens), tokens.size(1))\n",
    "    size_ta = max(len(audio_generated_tokens), tokens.size(1))\n",
    "    audio_token_error = (sum([1 for b, t in zip(audio_generated_tokens[:end_ta], tokens[0, :end_ta].tolist()) if b != t]) + size_ta - end_ta) / size_ta\n",
    "\n",
    "    # Backpropagation\n",
    "    loss = beta * align_mse + (1 - beta) * logit_kld * 10\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, brain_seq2seq_loss, audio_seq2seq_loss, logit_loss, logit_mse, logit_kld, align_mse, align_cos_time, align_cos_feat, brain_generated_tokens, audio_generated_tokens, token_disagreement, token_error, audio_token_error\n",
    "\n",
    "\n",
    "def train_full(model, data_loader, optimizer, beta, sampling_prob=0.1):\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    model.train()\n",
    "    batches = len(data_loader)\n",
    "    i = 0\n",
    "    log = {\n",
    "        \"text\": [],\n",
    "        \"length\": [],\n",
    "        \"loss\": [],\n",
    "        \"audio_seq_loss\": [],\n",
    "        \"brain_seq_loss\": [],\n",
    "        \"align_mse\": [],\n",
    "        \"align_cos_feat\": [],\n",
    "        \"align_cos_time\": [],\n",
    "        \"logit_was\": [],\n",
    "        \"logit_mse\": [],\n",
    "        \"logit_kldiv\": [],\n",
    "        \"token_disagreement\": [],\n",
    "        \"token_error\": [],\n",
    "        \"audio_token_error\": []\n",
    "    }\n",
    "    for brain_data, mels, texts, signal_lengths in data_loader:\n",
    "        loss, brain_seq2seq_loss, audio_seq2seq_loss, align_mse, align_cos_time, align_cos_feat, logit_loss, logit_mse, logit_kld, brain_generated_tokens, audio_generated_tokens, token_disagreement, token_error, audio_token_error = train_step_with_scheduled_sampling(\n",
    "            brain_data=brain_data,\n",
    "            mels=mels,\n",
    "            texts=texts,\n",
    "            optimizer=optimizer,\n",
    "            model=model,\n",
    "            beta=beta,\n",
    "            sampling_prob=sampling_prob\n",
    "        )\n",
    "        # Check token diversity\n",
    "        brain_token_counter = Counter(brain_generated_tokens)\n",
    "        audio_token_counter = Counter(audio_generated_tokens)\n",
    "\n",
    "        brain_unique_tokens = len(brain_token_counter)\n",
    "        audio_unique_tokens = len(audio_token_counter)\n",
    "\n",
    "        most_common_brain_token, brain_token_count = brain_token_counter.most_common(1)[0]\n",
    "        most_common_audio_token, audio_token_count = audio_token_counter.most_common(1)[0]\n",
    "\n",
    "        #print(f\"Batch {i+1}/{batches}: Brain unique tokens: {brain_unique_tokens}, Most common: {most_common_brain_token} ({brain_token_count} / {len(brain_generated_tokens)})\")\n",
    "        #print(f\"Batch {i+1}/{batches}: Audio unique tokens: {audio_unique_tokens}, Most common: {most_common_audio_token} ({audio_token_count} / {len(audio_generated_tokens)})\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            torch.save(model.state_dict(), checkpoint_dir + f\"\\{timestamp}-{i}.pt\")\n",
    "            try:\n",
    "                df = pd.DataFrame(log)\n",
    "                df.to_csv(checkpoint_dir + f\"\\{timestamp}-log.csv\", mode='a', header=False, index=False)\n",
    "            except Exception as e:\n",
    "                message = f\"Error saving log \\n{log}\\n {e}\"\n",
    "                raise Exception(message)\n",
    "        log = {\n",
    "            \"text\": [],\n",
    "            \"length\": [],\n",
    "            \"loss\": [],\n",
    "            \"audio_seq_loss\": [],\n",
    "            \"brain_seq_loss\": [],\n",
    "            \"align_mse\": [],\n",
    "            \"align_cos_feat\": [],\n",
    "            \"align_cos_time\": [],\n",
    "            \"logit_was\": [],\n",
    "            \"logit_mse\": [],\n",
    "            \"logit_kldiv\": [],\n",
    "            \"token_disagreement\": [],\n",
    "            \"token_error\": [],\n",
    "            \"audio_token_error\": []\n",
    "        }\n",
    "        i += 1\n",
    "        log[\"text\"] += [texts[0]]\n",
    "        log[\"length\"] += [(signal_lengths.item()*20)/1000]\n",
    "        log[\"loss\"] += [loss.item()]\n",
    "        log[\"audio_seq_loss\"] += [audio_seq2seq_loss.item()]\n",
    "        log[\"brain_seq_loss\"] += [brain_seq2seq_loss.item()]\n",
    "        log[\"align_mse\"] += [align_mse]\n",
    "        log[\"align_cos_feat\"] += [align_cos_feat]\n",
    "        log[\"align_cos_time\"] += [align_cos_time]\n",
    "        log[\"logit_was\"] += [logit_loss.item()]\n",
    "        log[\"logit_mse\"] += [logit_mse.item()]\n",
    "        log[\"logit_kldiv\"] += [logit_kld.item()]\n",
    "        log[\"token_disagreement\"] += [token_disagreement]\n",
    "        log[\"token_error\"] += [token_error]\n",
    "        log[\"audio_token_error\"] += [audio_token_error]\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f} ({i}/{batches}) align: {align_mse.item():.4f} logit: {logit_kld*10:.4f}\")\n",
    "    df = pd.DataFrame(log)\n",
    "    df.to_csv(checkpoint_dir + f\"\\{timestamp}-log.csv\", mode='a', header=False, index=False)\n",
    "    torch.save(model.state_dict(), checkpoint_dir + f\"\\{timestamp}-final.pt\")\n",
    "    return log, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load checkpoint\n",
    "checkpoint = torch.load(checkpoint_dir + '/20240529063848-final.pt')\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8632 (1/3460) align: 0.0427 logit: 0.0067\n",
      "Loss: 0.9072 (2/3460) align: 0.0549 logit: 0.0069\n",
      "Loss: 0.6970 (3/3460) align: 0.0338 logit: 0.0084\n",
      "Loss: 1.8975 (4/3460) align: 0.1113 logit: 0.0087\n",
      "Loss: 1.2560 (5/3460) align: 0.0995 logit: 0.0060\n",
      "Loss: 0.8794 (6/3460) align: 0.0893 logit: 0.0066\n",
      "Loss: 0.7782 (7/3460) align: 0.0694 logit: 0.0057\n",
      "Loss: 0.8075 (8/3460) align: 0.0450 logit: 0.0088\n",
      "Loss: 0.7468 (9/3460) align: 0.0352 logit: 0.0050\n",
      "Loss: 0.6189 (10/3460) align: 0.0327 logit: 0.0068\n",
      "Loss: 0.8085 (11/3460) align: 0.0313 logit: 0.0064\n",
      "Loss: 0.6643 (12/3460) align: 0.0552 logit: 0.0079\n",
      "Loss: 0.6293 (13/3460) align: 0.0316 logit: 0.0065\n",
      "Loss: 0.8547 (14/3460) align: 0.0607 logit: 0.0073\n",
      "Loss: 0.5996 (15/3460) align: 0.0274 logit: 0.0081\n",
      "Loss: 0.5374 (16/3460) align: 0.0259 logit: 0.0081\n",
      "Loss: 0.5252 (17/3460) align: 0.0209 logit: 0.0081\n",
      "Loss: 0.7090 (18/3460) align: 0.0444 logit: 0.0044\n",
      "Loss: 0.4631 (19/3460) align: 0.0292 logit: 0.0062\n",
      "Loss: 0.8922 (20/3460) align: 0.0415 logit: 0.0060\n",
      "Loss: 0.3646 (21/3460) align: 0.0177 logit: 0.0049\n",
      "Loss: 0.7682 (22/3460) align: 0.0329 logit: 0.0043\n",
      "Loss: 0.5573 (23/3460) align: 0.0374 logit: 0.0076\n",
      "Loss: 1.5520 (24/3460) align: 0.0372 logit: 0.0064\n",
      "Loss: 0.4662 (25/3460) align: 0.0467 logit: 0.0077\n",
      "Loss: 0.6262 (26/3460) align: 0.0173 logit: 0.0078\n",
      "Loss: 0.9234 (27/3460) align: 0.0481 logit: 0.0079\n",
      "Loss: 0.8976 (28/3460) align: 0.0232 logit: 0.0071\n",
      "Loss: 1.7546 (29/3460) align: 0.0406 logit: 0.0043\n",
      "Loss: 1.4797 (30/3460) align: 0.0515 logit: 0.0091\n",
      "Loss: 0.4274 (31/3460) align: 0.0128 logit: 0.0084\n",
      "Loss: 0.5846 (32/3460) align: 0.0355 logit: 0.0067\n",
      "Loss: 0.7949 (33/3460) align: 0.0372 logit: 0.0054\n",
      "Loss: 0.7168 (34/3460) align: 0.0416 logit: 0.0065\n",
      "Loss: 0.4447 (35/3460) align: 0.0335 logit: 0.0055\n",
      "Loss: 0.3676 (36/3460) align: 0.0297 logit: 0.0067\n",
      "Loss: 0.7547 (37/3460) align: 0.0444 logit: 0.0071\n",
      "Loss: 0.6154 (38/3460) align: 0.0626 logit: 0.0077\n",
      "Loss: 0.8154 (39/3460) align: 0.0306 logit: 0.0077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m      6\u001b[0m sampling_prob \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m----> 7\u001b[0m log, run_timestamp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_prob\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 144\u001b[0m, in \u001b[0;36mtrain_full\u001b[1;34m(model, data_loader, optimizer, beta, sampling_prob)\u001b[0m\n\u001b[0;32m    127\u001b[0m log \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_token_error\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m    142\u001b[0m }\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m brain_data, mels, texts, signal_lengths \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m--> 144\u001b[0m     loss, brain_seq2seq_loss, audio_seq2seq_loss, align_mse, align_cos_time, align_cos_feat, logit_loss, logit_mse, logit_kld, brain_generated_tokens, audio_generated_tokens, token_disagreement, token_error, audio_token_error \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_with_scheduled_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_prob\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Check token diversity\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     brain_token_counter \u001b[38;5;241m=\u001b[39m Counter(brain_generated_tokens)\n",
      "Cell \u001b[1;32mIn[36], line 54\u001b[0m, in \u001b[0;36mtrain_step_with_scheduled_sampling\u001b[1;34m(brain_data, mels, texts, optimizer, model, beta, sampling_prob)\u001b[0m\n\u001b[0;32m     52\u001b[0m align_mse, align_cos_time, align_cos_feat \u001b[38;5;241m=\u001b[39m alignment_loss(brain_embeddings, audio_embeddings)\n\u001b[0;32m     53\u001b[0m texts \u001b[38;5;241m=\u001b[39m clean_texts(texts)\n\u001b[1;32m---> 54\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mencode_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Start and end tokens\u001b[39;00m\n\u001b[0;32m     57\u001b[0m start_token \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mencode_texts\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m token_map:\n\u001b[0;32m     22\u001b[0m     token_array\u001b[38;5;241m.\u001b[39mappend(sots \u001b[38;5;241m+\u001b[39m tokens \u001b[38;5;241m+\u001b[39m eot)\n\u001b[1;32m---> 23\u001b[0m token_array \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token_array\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.toggle_freeze(part='whisper', unfreeze=False)\n",
    "lr = 0.0001\n",
    "sampling_prob = 0\n",
    "for i in range(0, 11):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    sampling_prob += i/20\n",
    "    log, run_timestamp = train_full(model, dataloader, optimizer, beta=0.5, sampling_prob=sampling_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from checkpoint\n",
    "checkpoint = torch.load(checkpoint_dir + \"\\\\20240528124126-final.pt\")\n",
    "model.toggle_freeze(part='audio', unfreeze=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "log, run_timestamp = train_full(model, dataloader, optimizer, loss_fn, mse_adjusted_temporal_gaussian_infonce_loss, beta=0.8, alpha=0.05, iqr=5, temperature=0.2, neg_sample_prop=7)\n",
    "tough_sentences = log.loc[(log[\"ASL\"] > 0.5) | (log[\"BSL\"] > 0.5)]\n",
    "tough_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model any time\n",
    "checkpoint_dir = \"K:\\ke\\sta\\data\\Willett&EtAl2023\\checkpoints\"\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "torch.save(model.state_dict(), checkpoint_dir + f\"\\{timestamp}-final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from whisper import DecodingOptions\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "from whisper.decoding import DecodingTask\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_function(model, brain_data, options):\n",
    "    result = DecodingTask(model, options).run(brain_data)\n",
    "    return result\n",
    "\n",
    "model.decode = decode_function\n",
    "\n",
    "testset = WillettDataset(\"K:\\ke\\sta\\data\\Willett&EtAl2023\\data\\Willett&EtAl2023.h5\", \"test\", device=DEVICE)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "options = DecodingOptions(\n",
    "    language=\"en\",\n",
    "    without_timestamps=True,\n",
    "    fp16=False\n",
    "    )\n",
    "normalizer = EnglishTextNormalizer()\n",
    "\n",
    "def decode_seq2seq(model, dataloader, options, sample_size=10):\n",
    "    model.eval()\n",
    "\n",
    "    brain_hypotheses = []\n",
    "    audio_hypotheses = []\n",
    "    references = []\n",
    "    batches = len(dataloader)\n",
    "    i = 0\n",
    "    for brain_data, mels, texts, signal_lengths in dataloader:\n",
    "\n",
    "        print(f\"Batch {i+1}/{batches} ({len(brain_hypotheses)} hypotheses so far)                        \", end=\"\\r\")\n",
    "\n",
    "        # Decode the neural data\n",
    "        model.toggle_mode(\"brain\")\n",
    "        neural_results = decode_function(model, brain_data, options)\n",
    "        brain_hypotheses.extend([result.text for result in neural_results])\n",
    "        model.toggle_mode(\"audio\")\n",
    "        audio_results = decode_function(model, mels, options)\n",
    "        audio_hypotheses.extend([result.text for result in audio_results])\n",
    "        references.extend(texts)\n",
    "\n",
    "        if i == sample_size:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    data = pd.DataFrame(dict(\n",
    "        brain_hypotheses=brain_hypotheses,\n",
    "        audio_hypotheses=audio_hypotheses,\n",
    "        reference=references))\n",
    "    data[\"brain_hypotheses_clean\"] = [normalizer(text) for text in data[\"brain_hypotheses\"]]\n",
    "    data[\"audio_hypotheses_clean\"] = [normalizer(text) for text in data[\"audio_hypotheses\"]]\n",
    "    data[\"reference_clean\"] = [normalizer(text) for text in data[\"reference\"]]\n",
    "    return data\n",
    "\n",
    "def wer(hypothesis, reference):\n",
    "    return jiwer.wer(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11/880 (10 hypotheses so far)                        \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brain_hypotheses</th>\n",
       "      <th>audio_hypotheses</th>\n",
       "      <th>reference</th>\n",
       "      <th>brain_hypotheses_clean</th>\n",
       "      <th>audio_hypotheses_clean</th>\n",
       "      <th>reference_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>Well tell me this.</td>\n",
       "      <td>Well tell me this.</td>\n",
       "      <td>thank you</td>\n",
       "      <td>well tell me this</td>\n",
       "      <td>well tell me this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>Clothing and gas</td>\n",
       "      <td>Clothing and gas.</td>\n",
       "      <td>thank you</td>\n",
       "      <td>clothing and gas</td>\n",
       "      <td>clothing and gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>What are we doing?</td>\n",
       "      <td>What are we doing?</td>\n",
       "      <td>thank you</td>\n",
       "      <td>what are we doing</td>\n",
       "      <td>what are we doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>I don't know how my father did it.</td>\n",
       "      <td>I don't know how my father did it.</td>\n",
       "      <td>thank you</td>\n",
       "      <td>i do not know how my father did it</td>\n",
       "      <td>i do not know how my father did it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>You can do it.</td>\n",
       "      <td>You could do it.</td>\n",
       "      <td>thank you</td>\n",
       "      <td>you can do it</td>\n",
       "      <td>you could do it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>I'll try that next week.</td>\n",
       "      <td>I'll try that next week.                      ...</td>\n",
       "      <td>thank you</td>\n",
       "      <td>i will try that next week</td>\n",
       "      <td>i will try that next week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>Friday afternoon at 5.30</td>\n",
       "      <td>Friday afternoon at five thirty.              ...</td>\n",
       "      <td>thank you</td>\n",
       "      <td>friday afternoon at 5.30</td>\n",
       "      <td>friday afternoon at 530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>Very well persuaded.</td>\n",
       "      <td>Very well persuaded.                          ...</td>\n",
       "      <td>thank you</td>\n",
       "      <td>very well persuaded</td>\n",
       "      <td>very well persuaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>Those answers will be straightforward if you t...</td>\n",
       "      <td>Those answers will be straightforward if you t...</td>\n",
       "      <td>thank you</td>\n",
       "      <td>those answers will be straightforward if you t...</td>\n",
       "      <td>those answers will be straightforward if you t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>Just about any kind of music.</td>\n",
       "      <td>Just about any kind of music.</td>\n",
       "      <td>thank you</td>\n",
       "      <td>just about any kind of music</td>\n",
       "      <td>just about any kind of music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>focal point.</td>\n",
       "      <td>Focal point.</td>\n",
       "      <td>thank you</td>\n",
       "      <td>focal point</td>\n",
       "      <td>focal point</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brain_hypotheses                                   audio_hypotheses  \\\n",
       "0        Thank you.                                 Well tell me this.   \n",
       "1        Thank you.                                   Clothing and gas   \n",
       "2        Thank you.                                 What are we doing?   \n",
       "3        Thank you.                 I don't know how my father did it.   \n",
       "4        Thank you.                                     You can do it.   \n",
       "5        Thank you.                           I'll try that next week.   \n",
       "6        Thank you.                           Friday afternoon at 5.30   \n",
       "7        Thank you.                               Very well persuaded.   \n",
       "8        Thank you.  Those answers will be straightforward if you t...   \n",
       "9        Thank you.                      Just about any kind of music.   \n",
       "10       Thank you.                                       focal point.   \n",
       "\n",
       "                                            reference brain_hypotheses_clean  \\\n",
       "0      Well tell me this.                                          thank you   \n",
       "1   Clothing and gas.                                              thank you   \n",
       "2   What are we doing?                                             thank you   \n",
       "3   I don't know how my father did it.                             thank you   \n",
       "4   You could do it.                                               thank you   \n",
       "5   I'll try that next week.                      ...              thank you   \n",
       "6   Friday afternoon at five thirty.              ...              thank you   \n",
       "7   Very well persuaded.                          ...              thank you   \n",
       "8   Those answers will be straightforward if you t...              thank you   \n",
       "9   Just about any kind of music.                                  thank you   \n",
       "10  Focal point.                                                   thank you   \n",
       "\n",
       "                               audio_hypotheses_clean  \\\n",
       "0                                   well tell me this   \n",
       "1                                    clothing and gas   \n",
       "2                                   what are we doing   \n",
       "3                  i do not know how my father did it   \n",
       "4                                       you can do it   \n",
       "5                           i will try that next week   \n",
       "6                            friday afternoon at 5.30   \n",
       "7                                 very well persuaded   \n",
       "8   those answers will be straightforward if you t...   \n",
       "9                        just about any kind of music   \n",
       "10                                        focal point   \n",
       "\n",
       "                                      reference_clean  \n",
       "0                                   well tell me this  \n",
       "1                                    clothing and gas  \n",
       "2                                   what are we doing  \n",
       "3                  i do not know how my father did it  \n",
       "4                                     you could do it  \n",
       "5                           i will try that next week  \n",
       "6                             friday afternoon at 530  \n",
       "7                                 very well persuaded  \n",
       "8   those answers will be straightforward if you t...  \n",
       "9                        just about any kind of music  \n",
       "10                                        focal point  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = decode_seq2seq(model, testloader, options, sample_size=10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain WER: 97.01 %\n",
      "Audio WER: 2.99 %\n"
     ]
    }
   ],
   "source": [
    "brain_wer = jiwer.wer(list(results[\"reference_clean\"]), list(results[\"brain_hypotheses_clean\"]))\n",
    "audio_wer = jiwer.wer(list(results[\"reference_clean\"]), list(results[\"audio_hypotheses_clean\"]))\n",
    "\n",
    "print(f\"Brain WER: {brain_wer * 100:.2f} %\")\n",
    "print(f\"Audio WER: {audio_wer * 100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
