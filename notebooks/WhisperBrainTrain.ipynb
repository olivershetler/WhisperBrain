{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.brain import initialize_from_whisper, WillettDataset, mse_adjusted_temporal_gaussian_infonce_loss\n",
    "import whisper\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Outline: Training WhisperBrain\n",
    "- Load data\n",
    "- Preprocess data\n",
    "- Train Contrastive Alignment of speech embeddings and brain embeddings\n",
    "- Train brain-to-text encoder-decoder.\n",
    "- Evaluate on test set\n",
    "- Save best checkpoint in a separate directory.\n",
    "- Transcribe holdout set and save to text file for submission (follow guidelines from competition)? Or do in other notebook?\n",
    "\n",
    "Reference for some finetuning code: https://colab.research.google.com/drive/1P4ClLkPmfsaKn2tBbRp0nVjGMRKR-EWz?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n",
      "Model is English-only and has 45,115,760 parameters.\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {'CUDA' if DEVICE.type == 'cuda' else 'CPU'}.\")\n",
    "model = initialize_from_whisper(name='tiny.en')\n",
    "model.to(DEVICE)\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "dataset = WillettDataset(\"K:\\ke\\sta\\data\\Willett&EtAl2023\\data\\Willett&EtAl2023.h5\", \"train\", device=DEVICE)\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "model.toggle_freeze('audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning Speech and Brain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop\n",
    "def train(model, data_loader, optimizer, alpha=0.5, iqr=20, temperature=0.07, neg_sample_prop=None):\n",
    "    model.train()\n",
    "    batches = len(data_loader)\n",
    "    i = 0\n",
    "    for brain_data, mels, texts, signal_lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass for brain and audio\n",
    "        brain_embeddings = model.embed_brain(brain_data)\n",
    "        audio_embeddings = model.embed_audio(mels)\n",
    "        # Compute losses\n",
    "        loss = mse_adjusted_temporal_gaussian_infonce_loss(brain_embeddings, audio_embeddings, signal_lengths, alpha=alpha, iqr=iqr, temperature=temperature, neg_sample_prop=neg_sample_prop)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        print(f\"Loss: {loss.item()} ({i}/{batches})         \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.26037299633026123 (199/3460)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miqr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sample_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, optimizer, alpha, iqr, temperature, neg_sample_prop)\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_adjusted_temporal_gaussian_infonce_loss(brain_embeddings, audio_embeddings, signal_lengths, alpha\u001b[38;5;241m=\u001b[39malpha, iqr\u001b[38;5;241m=\u001b[39miqr, temperature\u001b[38;5;241m=\u001b[39mtemperature, neg_sample_prop\u001b[38;5;241m=\u001b[39mneg_sample_prop)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[0;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, dataloader, optimizer, alpha=0.1, iqr=1, temperature=0.2, neg_sample_prop=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "checkpoint_dir = \"K:\\ke\\sta\\data\\Willett&EtAl2023\\checkpoints\"\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "torch.save(model.state_dict(), checkpoint_dir + f\"\\{timestamp}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Brain Encoder with Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50258, 50259, 50359, 50363,    40,   390, 11679,   538,   552,    13,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
      "           220,   220,   220,   220,   220, 50257]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(\"en\")\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "brain_data, mels, texts, signal_lengths = dataloader.__iter__().__next__()\n",
    "def encode_texts(texts):\n",
    "    # remove white space from start and end\n",
    "    for sentence in texts:\n",
    "        sentence = sentence.strip()\n",
    "    sots = list(tokenizer.sot_sequence_including_notimestamps)\n",
    "    eot = [tokenizer.eot]\n",
    "    token_map = map(tokenizer.encode, texts)\n",
    "    token_array = []\n",
    "    for tokens in token_map:\n",
    "        token_array.append(sots + tokens + eot)\n",
    "    token_array = torch.tensor(token_array, dtype=torch.long, device=DEVICE)\n",
    "    return token_array\n",
    "tokens = encode_texts(texts)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, data_loader, optimizer, loss_fn, tokenizer):\n",
    "    model.train()\n",
    "    batches = len(data_loader)\n",
    "    i = 0\n",
    "    for brain_data, mels, texts, signal_lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass for brain and audio\n",
    "        brain_embeddings = model.embed_brain(brain_data)\n",
    "        tokens = encode_texts(texts)\n",
    "        logits = model.logits(tokens, brain_embeddings)\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), tokens.view(-1))\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        print(f\"Loss: {loss.item()} ({i}/{batches})\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "checkpoint_dir = \"K:\\ke\\sta\\data\\Willett&EtAl2023\\checkpoints\"\n",
    "\n",
    "def train_step_with_scheduled_sampling(brain_data, mels, texts, signal_lengths, optimizer, model, seq2seq_loss, beta, alpha=0.5, iqr=20, temperature=0.07, neg_sample_prop=None, max_length=150, sampling_prob=0.1):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass for brain and audio\n",
    "    brain_embeddings = model.embed_brain(brain_data)\n",
    "    audio_embeddings = model.embed_audio(mels)\n",
    "    # Compute losses\n",
    "    alignment_loss = mse_adjusted_temporal_gaussian_infonce_loss(brain_embeddings, audio_embeddings, signal_lengths, alpha=alpha, iqr=iqr, temperature=temperature, neg_sample_prop=neg_sample_prop)\n",
    "    tokens = encode_texts(texts)\n",
    "\n",
    "    # Start and end tokens\n",
    "    start_token = tokens[0, 0].item()\n",
    "    end_token = tokens[0, -1].item()\n",
    "\n",
    "    brain_seq2seq_loss = 0\n",
    "    audio_seq2seq_loss = 0\n",
    "\n",
    "    brain_generated_tokens = [start_token]\n",
    "    audio_generated_tokens = [start_token]\n",
    "\n",
    "    for t in range(1, tokens.size(1)):\n",
    "        if random.random() < sampling_prob and t > 1:\n",
    "            brain_input = torch.tensor([brain_generated_tokens], device=brain_embeddings.device)\n",
    "            audio_input = torch.tensor([audio_generated_tokens], device=audio_embeddings.device)\n",
    "        else:\n",
    "            brain_input = tokens[:, :t]\n",
    "            audio_input = tokens[:, :t]\n",
    "\n",
    "        brain_logits = model.logits(brain_input, brain_embeddings)\n",
    "        audio_logits = model.logits(audio_input, audio_embeddings)\n",
    "\n",
    "        brain_next_token = brain_logits[:, -1].argmax(dim=-1).item()\n",
    "        audio_next_token = audio_logits[:, -1].argmax(dim=-1).item()\n",
    "\n",
    "        brain_generated_tokens.append(brain_next_token)\n",
    "        audio_generated_tokens.append(audio_next_token)\n",
    "\n",
    "        brain_seq2seq_loss += seq2seq_loss(brain_logits[:, -1, :], tokens[:, t])\n",
    "        audio_seq2seq_loss += seq2seq_loss(audio_logits[:, -1, :], tokens[:, t])\n",
    "\n",
    "    brain_seq2seq_loss /= tokens.size(1)\n",
    "    audio_seq2seq_loss /= tokens.size(1)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss = beta * (brain_seq2seq_loss + audio_seq2seq_loss) / 2 + (1 - beta) * alignment_loss\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, brain_seq2seq_loss, audio_seq2seq_loss, alignment_loss, brain_generated_tokens, audio_generated_tokens\n",
    "\n",
    "\n",
    "def train_full(model, data_loader, optimizer, seq2seq_loss, alignment_loss, beta, alpha=0.5, iqr=20, temperature=0.07, neg_sample_prop=None, max_length=150, sampling_prob=0.1):\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    model.train()\n",
    "    batches = len(data_loader)\n",
    "    i = 0\n",
    "    log = {\n",
    "        \"text\": [],\n",
    "        \"length\": [],\n",
    "        \"loss\": [],\n",
    "        \"ASL\": [],\n",
    "        \"BSL\": [],\n",
    "        \"ACL\": [],\n",
    "    }\n",
    "    for brain_data, mels, texts, signal_lengths in data_loader:\n",
    "        loss, brain_seq2seq_loss, audio_seq2seq_loss, alignment_loss, brain_generated_tokens, audio_generated_tokens = train_step_with_scheduled_sampling(\n",
    "            brain_data=brain_data,\n",
    "            mels=mels,\n",
    "            texts=texts,\n",
    "            signal_lengths=signal_lengths,\n",
    "            optimizer=optimizer,\n",
    "            model=model,\n",
    "            seq2seq_loss=seq2seq_loss,\n",
    "            beta=beta,\n",
    "            alpha=alpha,\n",
    "            iqr=iqr,\n",
    "            temperature=temperature,\n",
    "            neg_sample_prop=neg_sample_prop,\n",
    "            max_length=150,\n",
    "            sampling_prob=sampling_prob\n",
    "        )\n",
    "        # Check token diversity\n",
    "        brain_token_counter = Counter(brain_generated_tokens)\n",
    "        audio_token_counter = Counter(audio_generated_tokens)\n",
    "\n",
    "        brain_unique_tokens = len(brain_token_counter)\n",
    "        audio_unique_tokens = len(audio_token_counter)\n",
    "\n",
    "        most_common_brain_token, brain_token_count = brain_token_counter.most_common(1)[0]\n",
    "        most_common_audio_token, audio_token_count = audio_token_counter.most_common(1)[0]\n",
    "\n",
    "        print(f\"Batch {i+1}/{batches}: Brain unique tokens: {brain_unique_tokens}, Most common: {most_common_brain_token} ({brain_token_count} / {len(brain_generated_tokens)})\")\n",
    "        print(f\"Batch {i+1}/{batches}: Audio unique tokens: {audio_unique_tokens}, Most common: {most_common_audio_token} ({audio_token_count} / {len(audio_generated_tokens)})\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            torch.save(model.state_dict(), checkpoint_dir + f\"\\{timestamp}-{i}.pth\")\n",
    "            df = pd.DataFrame(log)\n",
    "            df.to_csv(checkpoint_dir + f\"\\{timestamp}-log.csv\", mode='a', header=False, index=False)\n",
    "            log = {\n",
    "                \"text\": [],\n",
    "                \"length\": [],\n",
    "                \"loss\": [],\n",
    "                \"ASL\": [],\n",
    "                \"BSL\": [],\n",
    "                \"ACL\": [],\n",
    "            }\n",
    "        i += 1\n",
    "        log[\"text\"] += [texts[0]]\n",
    "        log[\"length\"] += [(signal_lengths.item()*20)/1000]\n",
    "        log[\"loss\"] += [loss.item()]\n",
    "        log[\"ASL\"] += [audio_seq2seq_loss.item()]\n",
    "        log[\"BSL\"] += [brain_seq2seq_loss.item()]\n",
    "        log[\"ACL\"] += [alignment_loss.item()]\n",
    "        print(f\"Loss: {loss.item()} ({i}/{batches}, BSL: {brain_seq2seq_loss}, ASL: {audio_seq2seq_loss} ACL: {alignment_loss})                          \\nLEN: {(signal_lengths.item()*20)/1000} s                        \\nTXT: {texts[0]}                                                                      \")\n",
    "    df = pd.DataFrame(log)\n",
    "    df.to_csv(checkpoint_dir + f\"\\{timestamp}-log.csv\", mode='a', header=False, index=False)\n",
    "    torch.save(model.state_dict(), checkpoint_dir + f\"\\{timestamp}-final.pth\")\n",
    "    return log, timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "#checkpoint = torch.load(\"K:\\ke\\sta\\data\\Willett&EtAl2023\\checkpoints\\\\20240527155310.pth\")\n",
    "#model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/3460: Brain unique tokens: 7, Most common: 220 (31 / 42)\n",
      "Batch 1/3460: Audio unique tokens: 7, Most common: 220 (32 / 42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0050287246704102 (1/3460, BSL: 1.6450364589691162, ASL: 1.6509501934051514 ACL: 0.3620641231536865)                          \n",
      "LEN: 6.46 s                        \n",
      "Batch 2/3460: Brain unique tokens: 7, Most common: 220 (35 / 49)                                                                      \n",
      "Batch 2/3460: Audio unique tokens: 7, Most common: 220 (41 / 49)\n",
      "Loss: 1.3242324590682983 (2/3460, BSL: 2.221036672592163, ASL: 1.9877849817276 ACL: 0.5440540313720703)                          \n",
      "LEN: 6.24 s                        \n",
      "Batch 3/3460: Brain unique tokens: 8, Most common: 13 (9 / 18)                                                                                        \n",
      "Batch 3/3460: Audio unique tokens: 8, Most common: 13 (9 / 18)\n",
      "Loss: 3.69403076171875 (3/3460, BSL: 6.870942115783691, ASL: 6.8098464012146 ACL: 0.5476672649383545)                          \n",
      "LEN: 9.1 s                        \n",
      "Batch 4/3460: Brain unique tokens: 7, Most common: 220 (47 / 60)                                                            \n",
      "Batch 4/3460: Audio unique tokens: 7, Most common: 220 (47 / 60)\n",
      "Loss: 0.8676826357841492 (4/3460, BSL: 1.2531189918518066, ASL: 1.2556287050247192 ACL: 0.4809914827346802)                          \n",
      "LEN: 7.02 s                        \n",
      "Batch 5/3460: Brain unique tokens: 7, Most common: 220 (8 / 20)                                                                                       \n",
      "Batch 5/3460: Audio unique tokens: 9, Most common: 220 (8 / 20)\n",
      "Loss: 2.353738307952881 (5/3460, BSL: 4.353527545928955, ASL: 4.1958513259887695 ACL: 0.432786762714386)                          \n",
      "LEN: 7.94 s                        \n",
      "Batch 6/3460: Brain unique tokens: 7, Most common: 220 (23 / 34)                                                              \n",
      "Batch 6/3460: Audio unique tokens: 7, Most common: 220 (23 / 34)\n",
      "Loss: 1.138077974319458 (6/3460, BSL: 1.86687433719635, ASL: 1.9024783372879028 ACL: 0.3914794921875)                          \n",
      "LEN: 5.4 s                        \n",
      "Batch 7/3460: Brain unique tokens: 8, Most common: 220 (26 / 38)                                                              \n",
      "Batch 7/3460: Audio unique tokens: 8, Most common: 220 (26 / 38)\n",
      "Loss: 1.3100969791412354 (7/3460, BSL: 2.2623021602630615, ASL: 2.211954116821289 ACL: 0.3830655813217163)                          \n",
      "LEN: 6.64 s                        \n",
      "Batch 8/3460: Brain unique tokens: 8, Most common: 220 (36 / 45)                                                                      \n",
      "Batch 8/3460: Audio unique tokens: 8, Most common: 220 (36 / 45)\n",
      "Loss: 0.8381285667419434 (8/3460, BSL: 1.3032134771347046, ASL: 1.2948306798934937 ACL: 0.3772350251674652)                          \n",
      "LEN: 6.44 s                        \n",
      "Batch 9/3460: Brain unique tokens: 6, Most common: 220 (28 / 33)                                                             \n",
      "Batch 9/3460: Audio unique tokens: 6, Most common: 220 (28 / 33)\n",
      "Loss: 1.2112752199172974 (9/3460, BSL: 2.064648151397705, ASL: 2.08841609954834 ACL: 0.34601834416389465)                          \n",
      "LEN: 5.08 s                        \n",
      "Batch 10/3460: Brain unique tokens: 8, Most common: 220 (40 / 51)                                                                 \n",
      "Batch 10/3460: Audio unique tokens: 8, Most common: 220 (43 / 51)\n",
      "Loss: 0.9076149463653564 (10/3460, BSL: 1.4084906578063965, ASL: 1.5100481510162354 ACL: 0.35596054792404175)                          \n",
      "LEN: 6.34 s                        \n",
      "Batch 11/3460: Brain unique tokens: 8, Most common: 220 (40 / 52)                                                                                \n",
      "Batch 11/3460: Audio unique tokens: 8, Most common: 220 (40 / 52)\n",
      "Loss: 1.0988380908966064 (11/3460, BSL: 1.8698394298553467, ASL: 1.8440266847610474 ACL: 0.34074312448501587)                          \n",
      "LEN: 6.0 s                        \n",
      "TXT: So I'd like to wipe them all out.                                                                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mtoggle_freeze(part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m, unfreeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m log, run_timestamp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmse_adjusted_temporal_gaussian_infonce_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miqr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sample_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m tough_sentences \u001b[38;5;241m=\u001b[39m log\u001b[38;5;241m.\u001b[39mloc[(log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m|\u001b[39m (log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBSL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)]\n\u001b[0;32m      5\u001b[0m tough_sentences\n",
      "Cell \u001b[1;32mIn[25], line 73\u001b[0m, in \u001b[0;36mtrain_full\u001b[1;34m(model, data_loader, optimizer, seq2seq_loss, alignment_loss, beta, alpha, iqr, temperature, neg_sample_prop, max_length, sampling_prob)\u001b[0m\n\u001b[0;32m     64\u001b[0m log \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACL\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     71\u001b[0m }\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m brain_data, mels, texts, signal_lengths \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m---> 73\u001b[0m     loss, brain_seq2seq_loss, audio_seq2seq_loss, alignment_loss, brain_generated_tokens, audio_generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_with_scheduled_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignal_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignal_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq2seq_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq2seq_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43miqr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miqr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneg_sample_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneg_sample_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_prob\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# Check token diversity\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     brain_token_counter \u001b[38;5;241m=\u001b[39m Counter(brain_generated_tokens)\n",
      "Cell \u001b[1;32mIn[25], line 52\u001b[0m, in \u001b[0;36mtrain_step_with_scheduled_sampling\u001b[1;34m(brain_data, mels, texts, signal_lengths, optimizer, model, seq2seq_loss, beta, alpha, iqr, temperature, neg_sample_prop, max_length, sampling_prob)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m beta \u001b[38;5;241m*\u001b[39m (brain_seq2seq_loss \u001b[38;5;241m+\u001b[39m audio_seq2seq_loss) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta) \u001b[38;5;241m*\u001b[39m alignment_loss\n\u001b[1;32m---> 52\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mk:\\ke\\dev\\WhisperBrain\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.toggle_freeze(part='audio', unfreeze=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "log, run_timestamp = train_full(model, dataloader, optimizer, loss_fn, mse_adjusted_temporal_gaussian_infonce_loss, beta=0.5, alpha=0.1, iqr=10, temperature=0.2, neg_sample_prop=7, max_length=150, sampling_prob=0.1)\n",
    "tough_sentences = log.loc[(log[\"ASL\"] > 0.5) | (log[\"BSL\"] > 0.5)]\n",
    "tough_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from checkpoint\n",
    "checkpoint = torch.load(checkpoint_dir + \"\\\\20240528124126-final.pth\")\n",
    "model.toggle_freeze(part='audio', unfreeze=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "log, run_timestamp = train_full(model, dataloader, optimizer, loss_fn, mse_adjusted_temporal_gaussian_infonce_loss, beta=0.8, alpha=0.05, iqr=5, temperature=0.2, neg_sample_prop=7)\n",
    "tough_sentences = log.loc[(log[\"ASL\"] > 0.5) | (log[\"BSL\"] > 0.5)]\n",
    "tough_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model any time\n",
    "checkpoint_dir = \"K:\\ke\\sta\\data\\Willett&EtAl2023\\checkpoints\"\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "torch.save(model.state_dict(), checkpoint_dir + f\"\\{timestamp}-final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from whisper import DecodingOptions\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "from whisper.decoding import DecodingTask\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_function(model, brain_data, options):\n",
    "    result = DecodingTask(model, options).run(brain_data)\n",
    "    return result\n",
    "\n",
    "model.decode = decode_function\n",
    "\n",
    "testset = WillettDataset(\"K:\\ke\\sta\\data\\Willett&EtAl2023\\data\\Willett&EtAl2023.h5\", \"test\", device=DEVICE)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "options = DecodingOptions(\n",
    "    language=\"en\",\n",
    "    without_timestamps=True,\n",
    "    fp16=False\n",
    "    )\n",
    "normalizer = EnglishTextNormalizer()\n",
    "\n",
    "def decode_seq2seq(model, dataloader, options, sample_size=10):\n",
    "    model.eval()\n",
    "\n",
    "    brain_hypotheses = []\n",
    "    audio_hypotheses = []\n",
    "    references = []\n",
    "    batches = len(dataloader)\n",
    "    i = 0\n",
    "    for brain_data, mels, texts, signal_lengths in dataloader:\n",
    "\n",
    "        print(f\"Batch {i+1}/{batches} ({len(brain_hypotheses)} hypotheses so far)                        \", end=\"\\r\")\n",
    "\n",
    "        # Decode the neural data\n",
    "        model.toggle_mode(\"brain\")\n",
    "        neural_results = decode_function(model, brain_data, options)\n",
    "        brain_hypotheses.extend([result.text for result in neural_results])\n",
    "        model.toggle_mode(\"audio\")\n",
    "        audio_results = decode_function(model, mels, options)\n",
    "        audio_hypotheses.extend([result.text for result in audio_results])\n",
    "        references.extend(texts)\n",
    "\n",
    "        if i == sample_size:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    data = pd.DataFrame(dict(\n",
    "        brain_hypotheses=brain_hypotheses,\n",
    "        audio_hypotheses=audio_hypotheses,\n",
    "        reference=references))\n",
    "    data[\"brain_hypotheses_clean\"] = [normalizer(text) for text in data[\"brain_hypotheses\"]]\n",
    "    data[\"audio_hypotheses_clean\"] = [normalizer(text) for text in data[\"audio_hypotheses\"]]\n",
    "    data[\"reference_clean\"] = [normalizer(text) for text in data[\"reference\"]]\n",
    "    return data\n",
    "\n",
    "def wer(hypothesis, reference):\n",
    "    return jiwer.wer(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101/880 (100 hypotheses so far)                        \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brain_hypotheses</th>\n",
       "      <th>audio_hypotheses</th>\n",
       "      <th>reference</th>\n",
       "      <th>brain_hypotheses_clean</th>\n",
       "      <th>audio_hypotheses_clean</th>\n",
       "      <th>reference_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>Reports and papers and that sort of thing.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>reports and papers and that sort of thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>That's what I was doing before.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>that is what i was doing before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>Everyone knows they're going to get it.       ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>everyone knows they are going to get it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>I thought that that was what celery seed was for.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>i thought that that was what celery seed was for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>I'm glad it's done.                           ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>i am glad it is done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>Solving the variables in the equation.        ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>solving the variables in the equation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>He's in first grade now.                      ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>he is in 1st grade now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>Since this time last year I've changed.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>since this time last year i have changed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>Two hour classes.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2 hour classes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh|&gt;&lt;|zh...</td>\n",
       "      <td>The jury is expected to announce its decision....</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>the jury is expected to announce its decision</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      brain_hypotheses  \\\n",
       "0    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "1    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "2    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "3    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "4    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "..                                                 ...   \n",
       "96   <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "97   <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "98   <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "99   <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "100  <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "\n",
       "                                      audio_hypotheses  \\\n",
       "0    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "1    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "2    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "3    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "4    <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "..                                                 ...   \n",
       "96   <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "97   <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "98   <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "99   <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "100  <|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh|><|zh...   \n",
       "\n",
       "                                             reference brain_hypotheses_clean  \\\n",
       "0     Reports and papers and that sort of thing.                                \n",
       "1    That's what I was doing before.                                            \n",
       "2    Everyone knows they're going to get it.       ...                          \n",
       "3    I thought that that was what celery seed was for.                          \n",
       "4    I'm glad it's done.                           ...                          \n",
       "..                                                 ...                    ...   \n",
       "96   Solving the variables in the equation.        ...                          \n",
       "97   He's in first grade now.                      ...                          \n",
       "98   Since this time last year I've changed.                                    \n",
       "99   Two hour classes.                                                          \n",
       "100  The jury is expected to announce its decision....                          \n",
       "\n",
       "    audio_hypotheses_clean                                   reference_clean  \n",
       "0                                  reports and papers and that sort of thing  \n",
       "1                                            that is what i was doing before  \n",
       "2                                    everyone knows they are going to get it  \n",
       "3                           i thought that that was what celery seed was for  \n",
       "4                                                       i am glad it is done  \n",
       "..                     ...                                               ...  \n",
       "96                                     solving the variables in the equation  \n",
       "97                                                    he is in 1st grade now  \n",
       "98                                  since this time last year i have changed  \n",
       "99                                                            2 hour classes  \n",
       "100                            the jury is expected to announce its decision  \n",
       "\n",
       "[101 rows x 6 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = decode_seq2seq(model, testloader, options, sample_size=100)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain WER: 100.00 %\n",
      "Audio WER: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "brain_wer = jiwer.wer(list(results[\"reference_clean\"]), list(results[\"brain_hypotheses_clean\"]))\n",
    "audio_wer = jiwer.wer(list(results[\"reference_clean\"]), list(results[\"audio_hypotheses_clean\"]))\n",
    "\n",
    "print(f\"Brain WER: {brain_wer * 100:.2f} %\")\n",
    "print(f\"Audio WER: {audio_wer * 100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
